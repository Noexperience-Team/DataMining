{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   
    
    "# Mahalanobis Distance\n",
  "\n",
    "____________________________________________________\n",
    "\n",
    "\n",
    "### Created by :\n",
    "\n",
    
    ">                        Bacem Abroug\n",
    "\n",
    
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution. It is an extremely useful metric having, excellent applications in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification. This research explains the intuition and the math with practical examples on three machine learning use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "> Introduction\n",
    "__________________________\n",
    "\n",
    ">2.What’s wrong with using Euclidean Distance for Multivariate data?\n",
    "\n",
    ">3.What is Mahalanobis Distance?\n",
    "\n",
    ">4.The math and intuition behind Mahalanobis Distance\n",
    "\n",
    ">5.How to compute Mahalanobis Distance in Python\n",
    "\n",
    ">6.Usecase 1: Multivariate outlier detection using Mahalanobis distance\n",
    "\n",
    ">7.Usecase 2: Mahalanobis Distance for Classification Problems\n",
    "\n",
    "\n",
    "> Conclusion\n",
    "_____________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point (vector) and a distribution.\n",
    "It has excellent applications in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification and more untapped use cases.\n",
    "\n",
    ">Considering its extremely useful applications, this metric is seldom discussed or used in stats or ML workflows. This post explains the why and the when to use Mahalanobis distance and then explains the intuition and the math with useful applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What’s wrong with using Euclidean Distance for Multivariate data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with the basics. Euclidean distance is the commonly used straight line distance between two points.\n",
    "\n",
    "If the two points are in a two-dimensional plane (meaning, you have two numeric columns (p) and (q)) in your dataset), then the Euclidean distance between the two points (p1, q1) and (p2, q2) is:\n",
    "![](images/1_simple_euclidean_distance_formula-min.png)\n",
    "\n",
    "This formula may be extended to as many dimensions you want:\n",
    "![](images/2_multivariate_euclidean_distance_formula-min.png)\n",
    "\n",
    "Well, Euclidean distance will work fine as long as the dimensions are equally weighted and are independent of each other.\n",
    "\n",
    "What do I mean by that?\n",
    "Let’s consider the following tables:\n",
    "![](images/tables_scale_comparison-min.png)\n",
    "\n",
    "The two tables above show the ‘area’ and ‘price’ of the same objects. Only the units of the variables change.\n",
    "\n",
    "Since both tables represent the same entities, the distance between any two rows, point A and point B should be the same. But Euclidean distance gives a different value even though the distances are technically the same in physical space.\n",
    "\n",
    "This can technically be overcome by scaling the variables, by computing the z-score (ex: (x – mean) / std) or make it vary within a particular range like between 0 and 1.\n",
    "\n",
    "But there is another major drawback.\n",
    "\n",
    "That is, if the dimensions (columns in your dataset) are correlated to one another, which is typically the case in real-world datasets, the Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.\n",
    "\n",
    "![](images/Mahalanobis_Distance_Usecase.jpg)\n",
    "\n",
    "The above image (on the right) is a simple scatterplot of two variables that are positively correlated with each other. That is, as the value of one variable (x-axis) increases, so does the value of the other variable (y-axis).\n",
    "\n",
    "The two points above are equally distant (Euclidean) from the center. But only one of them (blue) is actually more close to the cluster, even though, technically the Euclidean distance between the two points are equal.\n",
    "This is because, Euclidean distance is a distance between two points only. It does not consider how the rest of the points in the dataset vary. So, it cannot be used to really judge how close a point actually is to a distribution of points.\n",
    "\n",
    ">>What we need here is a more robust distance metric that is an accurate representation of how distant a point is from a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is Mahalanobis Distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalonobis distance is the distance between a point and a distribution. And not between two distinct points. It is effectively a multivariate equivalent of the Euclidean distance.\n",
    "\n",
    "It was introduced by Prof. P. C. Mahalanobis in 1936 and has been used in various statistical applications ever since. However, it’s not so well known or used in the machine learning practice. Well, let’s get into it.\n",
    "\n",
    "So computationally, how is Mahalanobis distance different from Euclidean distance?\n",
    "\n",
    "1. It transforms the columns into uncorrelated variables\n",
    "2. Scale the columns to make their variance equal to 1\n",
    "3. Finally, it calculates the Euclidean distance.\n",
    "\n",
    "The above three steps are meant to address the problems with Euclidean distance we just talked about. But how?\n",
    "\n",
    ">> Let’s look at the formula and try to understand its components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The math and intuition behind Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula to compute Mahalanobis distance is as follows:\n",
    "![](images/3_Mahalanobis_Distance_Formula-min.png)\n",
    ">where, \n",
    " >>- D^2        is the square of the Mahalanobis distance. \n",
    " >>- x          is the vector of the observation (row in a dataset), \n",
    " >>- m          is the vector of mean values of independent variables (mean of each column), \n",
    " >>- C^(-1)     is the inverse covariance matrix of independent variables.\n",
    "__________________________________________________\n",
    "\n",
    "So, how to understand the above formula?\n",
    "Let’s take the (x – m)^T . C^(-1) term.\n",
    "\n",
    "(x – m) is essentially the distance of the vector from the mean. We then divide this by the covariance matrix (or multiply by the inverse of the covariance matrix).\n",
    "\n",
    "If you think about it, this is essentially a multivariate equivalent of the regular standardization (z = (x – mu)/sigma). That is, z = (x vector) – (mean vector) / (covariance matrix).\n",
    "\n",
    "So, What is the effect of dividing by the covariance?\n",
    "\n",
    "If the variables in your dataset are strongly correlated, then, the covariance will be high. Dividing by a large covariance will effectively reduce the distance.\n",
    "\n",
    "Likewise, if the X’s are not correlated, then the covariance is not high and the distance is not reduced much.\n",
    "\n",
    "So effectively, it addresses both the problems of scale as well as the correlation of the variables that we talked about in the introduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How to compute Mahalanobis Distance in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>61.5</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>59.8</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>62.4</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>63.3</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  depth  price\n",
       "0   0.23   61.5    326\n",
       "1   0.21   59.8    326\n",
       "2   0.23   56.9    327\n",
       "3   0.29   62.4    334\n",
       "4   0.31   63.3    335"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "filepath = 'https://raw.githubusercontent.com/selva86/datasets/master/diamonds.csv'\n",
    "df = pd.read_csv(filepath).iloc[:, [0,4,6]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s write the function to calculate Mahalanobis Distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>price</th>\n",
       "      <th>mahala</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>61.5</td>\n",
       "      <td>326</td>\n",
       "      <td>1.709860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>59.8</td>\n",
       "      <td>326</td>\n",
       "      <td>3.540097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>327</td>\n",
       "      <td>12.715021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>62.4</td>\n",
       "      <td>334</td>\n",
       "      <td>1.454469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>63.3</td>\n",
       "      <td>335</td>\n",
       "      <td>2.347239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat  depth  price     mahala\n",
       "0   0.23   61.5    326   1.709860\n",
       "1   0.21   59.8    326   3.540097\n",
       "2   0.23   56.9    327  12.715021\n",
       "3   0.29   62.4    334   1.454469\n",
       "4   0.31   63.3    335   2.347239"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mahalanobis(x=None, data=None, cov=None):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n",
    "    x    : vector or matrix of data with, say, p columns.\n",
    "    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n",
    "    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(data)\n",
    "    if not cov:\n",
    "        cov = np.cov(data.values.T)\n",
    "    inv_covmat = sp.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    return mahal.diagonal()\n",
    "\n",
    "df_x = df[['carat', 'depth', 'price']].head(500)\n",
    "df_x['mahala'] = mahalanobis(x=df_x, data=df[['carat', 'depth', 'price']])\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Usecase 1: Multivariate outlier detection using Mahalanobis distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the test statistic follows chi-square distributed with ‘n’ degree of freedom, the critical value at a 0.01 significance level and 2 degrees of freedom is computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.21034037197618"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Critical values for two degrees of freedom\n",
    "from scipy.stats import chi2\n",
    "chi2.ppf((1-0.01), df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That mean an observation can be considered as extreme if its Mahalanobis distance exceeds 9.21.\n",
    "\n",
    "If you prefer P values instead to determine if an observation is extreme or not, the P values can be computed as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>price</th>\n",
       "      <th>mahala</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>56.9</td>\n",
       "      <td>327</td>\n",
       "      <td>12.715021</td>\n",
       "      <td>0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.86</td>\n",
       "      <td>55.1</td>\n",
       "      <td>2757</td>\n",
       "      <td>23.909643</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.96</td>\n",
       "      <td>66.3</td>\n",
       "      <td>2759</td>\n",
       "      <td>11.781773</td>\n",
       "      <td>0.002765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.17</td>\n",
       "      <td>60.2</td>\n",
       "      <td>2774</td>\n",
       "      <td>9.279459</td>\n",
       "      <td>0.009660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.98</td>\n",
       "      <td>67.9</td>\n",
       "      <td>2777</td>\n",
       "      <td>20.086616</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.70</td>\n",
       "      <td>57.2</td>\n",
       "      <td>2782</td>\n",
       "      <td>10.405659</td>\n",
       "      <td>0.005501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.84</td>\n",
       "      <td>55.1</td>\n",
       "      <td>2782</td>\n",
       "      <td>23.548379</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.05</td>\n",
       "      <td>65.8</td>\n",
       "      <td>2789</td>\n",
       "      <td>11.237146</td>\n",
       "      <td>0.003630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1.00</td>\n",
       "      <td>58.2</td>\n",
       "      <td>2795</td>\n",
       "      <td>10.349019</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1.01</td>\n",
       "      <td>67.4</td>\n",
       "      <td>2797</td>\n",
       "      <td>17.716144</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     carat  depth  price     mahala   p_value\n",
       "2     0.23   56.9    327  12.715021  0.001734\n",
       "91    0.86   55.1   2757  23.909643  0.000006\n",
       "97    0.96   66.3   2759  11.781773  0.002765\n",
       "172   1.17   60.2   2774   9.279459  0.009660\n",
       "204   0.98   67.9   2777  20.086616  0.000043\n",
       "221   0.70   57.2   2782  10.405659  0.005501\n",
       "227   0.84   55.1   2782  23.548379  0.000008\n",
       "255   1.05   65.8   2789  11.237146  0.003630\n",
       "284   1.00   58.2   2795  10.349019  0.005659\n",
       "298   1.01   67.4   2797  17.716144  0.000142"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the P-Values\n",
    "df_x['p_value'] = 1 - chi2.cdf(df_x['mahala'], 2)\n",
    "\n",
    "# Extreme values with a significance level of 0.01\n",
    "df_x.loc[df_x.p_value < 0.01].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the above observations against rest of the dataset, they are clearly extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usecase 2: Mahalanobis Distance for Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahalanobis distance can be used for classification problems. A naive implementation of a Mahalanobis classifier is coded below. The intuition is that, an observation is assigned the class that it is closest to based on the Mahalanobis distance.\n",
    "\n",
    "Let’s see an example implementation on the BreastCancer dataset, where the objective is to determine if a tumour is benign or malignant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cl.thickness</th>\n",
       "      <th>Cell.size</th>\n",
       "      <th>Marg.adhesion</th>\n",
       "      <th>Epith.c.size</th>\n",
       "      <th>Bare.nuclei</th>\n",
       "      <th>Bl.cromatin</th>\n",
       "      <th>Normal.nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cl.thickness  Cell.size  Marg.adhesion  Epith.c.size  Bare.nuclei  \\\n",
       "0             5          1              1             2          1.0   \n",
       "1             5          4              5             7         10.0   \n",
       "2             3          1              1             2          2.0   \n",
       "3             6          8              1             3          4.0   \n",
       "4             4          1              3             2          1.0   \n",
       "\n",
       "   Bl.cromatin  Normal.nucleoli  Mitoses  Class  \n",
       "0            3                1        1      0  \n",
       "1            3                2        1      0  \n",
       "2            3                1        1      0  \n",
       "3            3                7        1      0  \n",
       "4            3                1        1      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BreastCancer.csv', \n",
    "                 usecols=['Cl.thickness', 'Cell.size', 'Marg.adhesion', \n",
    "                          'Epith.c.size', 'Bare.nuclei', 'Bl.cromatin', 'Normal.nucleoli', \n",
    "                          'Mitoses', 'Class'])\n",
    "\n",
    "df.dropna(inplace=True)  # drop missing values.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s split the dataset in 70:30 ratio as Train and Test. And the training dataset is split into homogeneous groups of ‘pos'(1) and ‘neg'(0) classes. To predict the class of the test dataset, we measure the Mahalanobis distances between a given observation (row) and both the positive (xtrain_pos) and negative datasets(xtrain_neg).\n",
    "\n",
    "Then that observation is assigned the class based on the group it is closest to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(df.drop('Class', axis=1), df['Class'], test_size=.3, random_state=100)\n",
    "\n",
    "# Split the training data as pos and neg\n",
    "xtrain_pos = xtrain.loc[ytrain == 1, :]\n",
    "xtrain_neg = xtrain.loc[ytrain == 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build the MahalanobiBinaryClassifier. To do that, you need to define the predict_proba() and the predict() methods. This classifier does not require a separate fit() (training) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pred  true\n",
      "0     0     0\n",
      "1     1     1\n",
      "2     0     0\n",
      "3     0     0\n",
      "4     0     0\n"
     ]
    }
   ],
   "source": [
    "class MahalanobisBinaryClassifier():\n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        self.xtrain_pos = xtrain.loc[ytrain == 1, :]\n",
    "        self.xtrain_neg = xtrain.loc[ytrain == 0, :]\n",
    "\n",
    "    def predict_proba(self, xtest):\n",
    "        pos_neg_dists = [(p,n) for p, n in zip(mahalanobis(xtest, self.xtrain_pos), mahalanobis(xtest, self.xtrain_neg))]\n",
    "        return np.array([(1-n/(p+n), 1-p/(p+n)) for p,n in pos_neg_dists])\n",
    "\n",
    "    def predict(self, xtest):\n",
    "        return np.array([np.argmax(row) for row in self.predict_proba(xtest)])\n",
    "\n",
    "\n",
    "clf = MahalanobisBinaryClassifier(xtrain, ytrain)        \n",
    "pred_probs = clf.predict_proba(xtest)\n",
    "pred_class = clf.predict(xtest)\n",
    "\n",
    "# Pred and Truth\n",
    "pred_actuals = pd.DataFrame([(pred, act) for pred, act in zip(pred_class, ytest)], columns=['pred', 'true'])\n",
    "print(pred_actuals[:5])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how the classifier performed on the test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC:  0.990974358974359\n",
      "\n",
      "Confusion Matrix: \n",
      " [[113  17]\n",
      " [  0  75]]\n",
      "\n",
      "Accuracy Score:  0.9170731707317074\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       130\n",
      "           1       0.82      1.00      0.90        75\n",
      "\n",
      "    accuracy                           0.92       205\n",
      "   macro avg       0.91      0.93      0.91       205\n",
      "weighted avg       0.93      0.92      0.92       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "truth = pred_actuals.loc[:, 'true']\n",
    "pred = pred_actuals.loc[:, 'pred']\n",
    "scores = np.array(pred_probs)[:, 1]\n",
    "print('AUROC: ', roc_auc_score(truth, scores))\n",
    "print('\\nConfusion Matrix: \\n', confusion_matrix(truth, pred))\n",
    "print('\\nAccuracy Score: ', accuracy_score(truth, pred))\n",
    "print('\\nClassification Report: \\n', classification_report(truth, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Mahalanobis distance alone is able to contribute to this much accuracy (92%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In this research, we covered nearly everything about Mahalanobis distance: the intuition behind the formula, the actual calculation in python and how it can be used for multivariate anomaly detection, binary classification, and one-class classification. It is known to perform really well when you have a highly imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
